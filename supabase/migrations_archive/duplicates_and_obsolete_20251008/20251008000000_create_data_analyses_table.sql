/*
  # Create data_analyses table - Simplified Analytics Architecture

  1. Purpose
    - Single source of truth for data analysis results
    - Replaces complex multi-table structure (datasets, dataset_rows, dataset_matrix, analyses)
    - Supports "Generic Intelligent Analyst" approach where LLM generates SQL dynamically

  2. New Tables
    - `data_analyses` - stores complete analysis lifecycle
      - `id` (uuid, primary key)
      - `user_id` (uuid, references auth.users)
      - `conversation_id` (uuid, references conversations)
      - `message_id` (uuid, references messages) - links to chat message
      - `file_hash` (text) - SHA-256 hash for caching (avoid reprocessing same file)
      - `file_metadata` (jsonb) - {original_filename, size_bytes, mime_type, rows_count, columns_count}
      - `parsed_schema` (jsonb) - {columns: [{name, type, sample_values, null_count}]}
      - `sample_data` (jsonb) - 50 rows sample for reference (NOT used for calculations)
      - `user_question` (text) - Original question from user
      - `llm_reasoning` (text) - LLM's explanation of analysis strategy
      - `generated_sql` (text) - SQL generated by LLM to answer question
      - `full_dataset_rows` (integer) - CRITICAL: Total rows in original file (calculations run on 100%)
      - `query_results` (jsonb) - Results from SQL execution on COMPLETE dataset
      - `ai_response` (jsonb) - {summary, insights, metrics, charts, recommendations}
      - `status` (text) - 'processing', 'completed', 'failed'
      - `error_message` (text, nullable)
      - `created_at` (timestamptz)
      - `updated_at` (timestamptz)

  3. Key Architecture Decision
    ⚠️ CRITICAL: SAMPLE vs COMPLETE DATA
    - LLM receives: `sample_data` (50 rows) - to understand structure and generate SQL
    - PostgreSQL executes: ALL rows via temp table - to calculate accurate results
    - Result: Low API cost + High accuracy

    Example:
    - File has 10,000 sales records
    - LLM sees 50 rows → understands columns: date, product, amount
    - LLM generates: SELECT SUM(amount) FROM temp_table WHERE date > '2024-01-01'
    - PostgreSQL runs on ALL 10,000 rows → returns accurate sum

  4. Indexes
    - Primary key on id
    - Index on user_id for fast user queries
    - Index on conversation_id for chat context
    - Index on file_hash for cache lookups
    - Index on status for filtering active analyses
    - Composite index on (user_id, created_at DESC) for recent analyses

  5. Security
    - Enable RLS on data_analyses
    - Users can only read/write their own analyses
    - Policies check auth.uid() = user_id

  6. Benefits of This Design
    ✅ Single table = simple RLS, no cascade complexity
    ✅ JSONB flexibility = supports any data format/question
    ✅ file_hash caching = instant results for duplicate files
    ✅ Preserves full context = debugging and re-analysis possible
    ✅ No intermediate tables = no permission errors between layers
*/

-- Create data_analyses table
CREATE TABLE IF NOT EXISTS data_analyses (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id uuid NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  conversation_id uuid REFERENCES conversations(id) ON DELETE CASCADE,
  message_id uuid REFERENCES messages(id) ON DELETE SET NULL,

  -- File identification and caching
  file_hash text NOT NULL,
  file_metadata jsonb NOT NULL DEFAULT '{}'::jsonb,

  -- Data structure (schema detected from full dataset)
  parsed_schema jsonb NOT NULL DEFAULT '{}'::jsonb,
  sample_data jsonb NOT NULL DEFAULT '[]'::jsonb,

  -- Analysis request and strategy
  user_question text NOT NULL,
  llm_reasoning text,
  generated_sql text,

  -- CRITICAL: This tracks the FULL dataset size
  -- The analysis runs on ALL rows, not just the sample
  full_dataset_rows integer NOT NULL DEFAULT 0,

  -- Results from execution on COMPLETE data
  query_results jsonb DEFAULT '[]'::jsonb,
  ai_response jsonb DEFAULT '{}'::jsonb,

  -- Status tracking
  status text NOT NULL DEFAULT 'processing' CHECK (status IN ('processing', 'completed', 'failed')),
  error_message text,

  -- Timestamps
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now()
);

-- Create indexes for performance
CREATE INDEX IF NOT EXISTS data_analyses_user_id_idx ON data_analyses(user_id);
CREATE INDEX IF NOT EXISTS data_analyses_conversation_id_idx ON data_analyses(conversation_id);
CREATE INDEX IF NOT EXISTS data_analyses_file_hash_idx ON data_analyses(file_hash);
CREATE INDEX IF NOT EXISTS data_analyses_status_idx ON data_analyses(status);
CREATE INDEX IF NOT EXISTS data_analyses_user_created_idx ON data_analyses(user_id, created_at DESC);

-- Enable RLS
ALTER TABLE data_analyses ENABLE ROW LEVEL SECURITY;

-- Drop existing policies if they exist (idempotent)
DROP POLICY IF EXISTS "Users can read own analyses" ON data_analyses;
DROP POLICY IF EXISTS "Users can insert own analyses" ON data_analyses;
DROP POLICY IF EXISTS "Users can update own analyses" ON data_analyses;
DROP POLICY IF EXISTS "Users can delete own analyses" ON data_analyses;

-- RLS Policies: Users can only access their own analyses
CREATE POLICY "Users can read own analyses"
  ON data_analyses
  FOR SELECT
  TO authenticated
  USING (auth.uid() = user_id);

CREATE POLICY "Users can insert own analyses"
  ON data_analyses
  FOR INSERT
  TO authenticated
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can update own analyses"
  ON data_analyses
  FOR UPDATE
  TO authenticated
  USING (auth.uid() = user_id)
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can delete own analyses"
  ON data_analyses
  FOR DELETE
  TO authenticated
  USING (auth.uid() = user_id);

-- Create updated_at trigger
CREATE OR REPLACE FUNCTION update_data_analyses_updated_at()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = now();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

DROP TRIGGER IF EXISTS update_data_analyses_updated_at_trigger ON data_analyses;
CREATE TRIGGER update_data_analyses_updated_at_trigger
  BEFORE UPDATE ON data_analyses
  FOR EACH ROW
  EXECUTE FUNCTION update_data_analyses_updated_at();

-- Add comment explaining the architecture
COMMENT ON TABLE data_analyses IS 'Simplified analytics table. LLM receives sample_data (50 rows) to understand structure, but generated_sql executes on full_dataset_rows (all data) for accurate results.';
COMMENT ON COLUMN data_analyses.sample_data IS 'Small sample (50 rows) sent to LLM for context - NOT used for calculations';
COMMENT ON COLUMN data_analyses.full_dataset_rows IS 'CRITICAL: Total rows in original file - SQL executes on 100% of this data';
COMMENT ON COLUMN data_analyses.query_results IS 'Results from SQL execution on COMPLETE dataset (all rows)';
